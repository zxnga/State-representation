{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gymnasium.spaces import Box\n",
    "from src.state_representation import DenseAutoEncoder, BaseForwardModel, CombinedModel, BaseInverseModel, BaseRewardModel\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "from src.ae_utils import (create_replay_buffer, populate_replay_buffer, get_action,\n",
    "                        extract_data_from_buffer, prepare_dataloaders, train_combined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate your ENV, modify this function to return you env gymnasium obkect representing your env\n",
    "env_name = '<ENV_NAME>'\n",
    "n_agent = 15\n",
    "random_seed = 0\n",
    "\n",
    "env, agents, simulation_start, simulation_end = setup_environment(env_name, n_agent, random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciate the replay buffer and add <SIZE> transitions \n",
    "size = <SIZE>\n",
    "\n",
    "buffer = create_replay_buffer(env, n_agent, size)\n",
    "populate_replay_buffer(env, buffer, size, n_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the type of model we want to use, here we are training with every auxiliary functions available\n",
    "type_ae = 'forward_inverse_reward'\n",
    "\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = input_dim\n",
    "state_dim = 90 # modify the size of the latent space as you wish\n",
    "action_dim = env.action_space.shape[0] # nb of actions from your env\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "test_size = 0.2\n",
    "validation_size = 0.1\n",
    "\n",
    "use_next_states = False # we use the prediction of the forward model\n",
    "\n",
    "plot_dir = f'./plots/{type_ae}/'\n",
    "weights_dir = f'./saved_models/{type_ae}/state_dim_{state_dim}/'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Instanciante auxilary models\n",
    "autoencoder = DenseAutoEncoder(input_dim, output_dim, state_dim).to(device)\n",
    "forward_model = BaseForwardModel().to(device)\n",
    "forward_model.initForwardNet(state_dim, action_dim, model_type='mlp')\n",
    "inverse_model = BaseInverseModel().to(device)\n",
    "inverse_model.initInverseNet(state_dim, action_dim, model_type=\"mlp\")\n",
    "reward_model = BaseRewardModel().to(device)\n",
    "reward_model.initRewardNet(state_dim, model_type=\"mlp\")\n",
    "combined_model = CombinedModel(autoencoder, forward_model, inverse_model, reward_model).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "reconstruction_criterion = nn.MSELoss()\n",
    "prediction_criterion = nn.MSELoss()\n",
    "inverse_criterion = nn.MSELoss()  # Assuming continuous actions\n",
    "reward_criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_splits) = extract_data_from_buffer(buffer, input_dim, action_dim,\n",
    "                                            test_size=0.2, validation_size=0.1)\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_dataloaders(*data_splits, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined_model_offline(combined_model, state_dim, train_loader, val_loader,\n",
    "        test_loader, num_epochs, reconstruction_criterion, prediction_criterion,\n",
    "        inverse_criterion, reward_criterion, optimizer, use_next_states,\n",
    "        plot_dir, weights_dir)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
